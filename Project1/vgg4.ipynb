{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saad Khan and Nithun Selva**\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "Project 1: Deep Neural Networks \n",
    "\n",
    "#### Week 1: VGG4 and building a deep learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 17:16:26.013422: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-09 17:16:26.024883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739139386.040487    7105 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739139386.045731    7105 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-09 17:16:26.061770: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Train `VGG4` on MNIST and CIFAR-10\n",
    "\n",
    "Woohoo! VGG4 is built and tested! Time for some fun! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train in a reasonable amount of time, upload your project code to CoCalc so that you can train on the GPU. See [instructions on the class website ](https://cs.colby.edu/courses/S25/cs444/software.html#cocalc) for getting this setup.\n",
    "\n",
    "*As noted on the website, if you have a higher-end Macbook with a Pro or Max chip and â‰¥ 16 GB of memory, you could probably run this workload on your computer if you would prefer. This is entirely optional and the cloud will almost certainly be faster than even the fastest Mac. Setting this up would just provide some extra convenience/flexibility. See the [macOS instructions](https://cs.colby.edu/courses/S25/cs444/software.html#tfmac) for setup.*\n",
    "\n",
    "Run the cell below to make sure TensorFlow is running on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print('Running on the GPU')\n",
    "else:\n",
    "    print('NOT running on the GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset\n",
    "from vgg_nets import VGG4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Train `VGG4` on MNIST on the GPU\n",
    "\n",
    "This will be a \"hello world\" test to make sure your `fit` method is working.\n",
    "\n",
    "Write code in the cell below to load in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739139387.882784    7105 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training set data have shape (54000, 28, 28, 1) and they should be (54000, 28, 28, 1)\n",
      "Your training set labels have shape (54000,) and they should be (54000,)\n",
      "Your val set data have shape (6000, 28, 28, 1) and they should be (6000, 28, 28, 1)\n",
      "Your val set labels have shape (6000,) and they should be (6000,)\n",
      "Your test set data have shape (10000, 28, 28, 1) and they should be (10000, 28, 28, 1)\n",
      "Your test set labels have shape (10000,) and they should be (10000,)\n"
     ]
    }
   ],
   "source": [
    "#load mnist data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset(\"mnist\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KEEP ME\n",
    "print(f'Your training set data have shape {x_train.shape} and they should be (54000, 28, 28, 1)')\n",
    "print(f'Your training set labels have shape {y_train.shape} and they should be (54000,)')\n",
    "print(f'Your val set data have shape {x_val.shape} and they should be (6000, 28, 28, 1)')\n",
    "print(f'Your val set labels have shape {y_val.shape} and they should be (6000,)')\n",
    "print(f'Your test set data have shape {x_test.shape} and they should be (10000, 28, 28, 1)')\n",
    "print(f'Your test set labels have shape {y_test.shape} and they should be (10000,)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `VGG4` in the cell below on MNIST for `7` epochs and a batch size of `1024`! Print out your accuracy on the test set when training is done.\n",
    "\n",
    "The **entire** process of training and evaluating test accuracy should take no more than 1 min (*at most!*). If it is taking longer, seek help.\n",
    "\n",
    "Your print outs should look something like:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 10]\n",
    "Dropout layer output(dropout1) shape: [1, 128]\n",
    "Dense layer output(dense1) shape: [1, 128]\n",
    "Flatten layer output(flat) shape: [1, 12544]\n",
    "MaxPool2D layer output(maxpool1) shape: [1, 14, 14, 64]\n",
    "Conv2D layer output(conv2) shape: [1, 28, 28, 64]\n",
    "Conv2D layer output(conv1) shape: [1, 28, 28, 64]\n",
    "---------------------------------------------------------------------------\n",
    "Epoch 0/6, Training loss 1.54, Val loss 0.33, Val acc 89.94 \n",
    "Epoch 0 took: blah0 secs\n",
    "Epoch 1/6, Training loss 0.42, Val loss 0.17, Val acc 94.59 \n",
    "Epoch 1 took: blah1 secs\n",
    "...\n",
    "Epoch 6/6, Training loss surprise, Val loss surprise, Val acc surprise\n",
    "Epoch 6 took: blah6 secs\n",
    "Finished training after 7 epochs!\n",
    "VGG4 MNIST Test accuracy: surprise%\n",
    "```\n",
    "\n",
    "The val and test accuracy should be satisfyingly high â€” in the high 90s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 10]\n",
      "Dropout layer output(Dropout) shape: [1, 128]\n",
      "Dense layer output(Dense_1) shape: [1, 128]\n",
      "Flatten layer output(Flatten) shape: [1, 12544]\n",
      "MaxPool2D layer output(MaxPool_1) shape: [1, 14, 14, 64]\n",
      "Conv2D layer output(Conv2D_2) shape: [1, 28, 28, 64]\n",
      "Conv2D layer output(Conv2D_1) shape: [1, 28, 28, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1/7 - Train Loss: 1.5436, Val Loss: 0.3262, Val Acc: 0.8985\n",
      "Epoch 1 completed in 3.28 seconds.\n",
      "Epoch 2/7 - Train Loss: 0.2725, Val Loss: 0.1527, Val Acc: 0.9546\n",
      "Epoch 2 completed in 2.03 seconds.\n",
      "Epoch 3/7 - Train Loss: 0.1668, Val Loss: 0.1302, Val Acc: 0.9600\n",
      "Epoch 3 completed in 2.00 seconds.\n",
      "Epoch 4/7 - Train Loss: 0.1277, Val Loss: 0.1058, Val Acc: 0.9694\n",
      "Epoch 4 completed in 2.02 seconds.\n",
      "Epoch 5/7 - Train Loss: 0.1063, Val Loss: 0.0971, Val Acc: 0.9709\n",
      "Epoch 5 completed in 1.98 seconds.\n",
      "Epoch 6/7 - Train Loss: 0.0894, Val Loss: 0.0931, Val Acc: 0.9703\n",
      "Epoch 6 completed in 1.91 seconds.\n",
      "Epoch 7/7 - Train Loss: 0.0743, Val Loss: 0.0802, Val Acc: 0.9761\n",
      "Epoch 7 completed in 1.90 seconds.\n",
      "Finished training after 7 epochs!\n",
      "Test Accuracy after forcing eval mode: 0.9706\n"
     ]
    }
   ],
   "source": [
    "# KEEP THIS SEED\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "#train vgg4 model on mnist for 7 epochs with batch size 1024, report the accuracy on the test set\n",
    "model = VGG4(C=10, input_feats_shape=(28, 28, 1))\n",
    "model.compile(optimizer='adam', loss='cross_entropy')\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(x_train, y_train, x_val, y_val, max_epochs=7, batch_size=1024)\n",
    "\n",
    "#calculate the accuracy on the test set\n",
    "model.set_layer_training_mode(False)  # Disable dropout & batch norm during evaluation\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Train `VGG4` on MNIST on the CPU\n",
    "\n",
    "To appreciate the advantage for training deep networks on GPUs instead of your computer's CPU, copy-paste your code above that trains your VGG4 net on MNIST below. Instead of running it on CoCalc, run it locally on your computer. Be sure to print out the time per epoch.\n",
    "\n",
    "*If the net takes >2 minutes per epoch on your computer, just train for one epoch then call it quits :)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Questions\n",
    "\n",
    "**Question 1:** Approximately how long did it take for 1 epoch of training with and without the GPU. Compute the relative compute time (`gpu_time_per_epoch`/`cpu_time_per_epoch`). What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Train `VGG4` on CIFAR-10\n",
    "\n",
    "Now let's train on CIFAR-10. Run this (*and all subsequent large training sessions*) on CoCalc/the GPU ðŸ˜Š\n",
    "\n",
    "Write code in the cell below to load in CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training set data have shape (54000, 28, 28, 1) and they should be (45000, 32, 32, 3)\n",
      "Your training set labels have shape (54000,) and they should be (45000,)\n",
      "Your val set data have shape (6000, 28, 28, 1) and they should be (5000, 32, 32, 3)\n",
      "Your val set labels have shape (6000,) and they should be (5000,)\n",
      "Your test set data have shape (10000, 28, 28, 1) and they should be (10000, 32, 32, 3)\n",
      "Your test set labels have shape (10000,) and they should be (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KEEP ME\n",
    "print(f'Your training set data have shape {x_train.shape} and they should be (45000, 32, 32, 3)')\n",
    "print(f'Your training set labels have shape {y_train.shape} and they should be (45000,)')\n",
    "print(f'Your val set data have shape {x_val.shape} and they should be (5000, 32, 32, 3)')\n",
    "print(f'Your val set labels have shape {y_val.shape} and they should be (5000,)')\n",
    "print(f'Your test set data have shape {x_test.shape} and they should be (10000, 32, 32, 3)')\n",
    "print(f'Your test set labels have shape {y_test.shape} and they should be (10000,)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `VGG4` on CIFAR-10 in the cell below for `15` epochs and use a batch size of `128`.\n",
    "\n",
    "*You should anticipate 10-20 secs per epoch of training. Thus, the whole training session should take ~2.5-5 mins. If this is far off, please seek help.*\n",
    "\n",
    "Plot the training and val loss over epochs. Put test acc in the title. *If everything is working, your training and validation loss should steadily decrease then start to plateau by the end of training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP THIS SEED\n",
    "tf.random.set_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e. Experiment: Effect of batch size on runtime and accuracy\n",
    "\n",
    "To develop your intuition about how the choice of batch size generally affects runtime and accuracy on a dataset like CIFAR-10, run an experiment in which fresh `VGG4` nets are trained for `15` epochs with one of the following batch sizes:<br/>\n",
    "`[128, 256, 512, 1024, 2048]`. After each training run, record the test accuracy and runtime.\n",
    "\n",
    "Create two plots:\n",
    "1. The test accuracy (y axis) as a function of the batch size (x axis). There should be 5 markers joined by a single curve.\n",
    "2. The runtime (y axis) as a function of the batch size (x axis). There should be 5 markers joined by a single curve.\n",
    "\n",
    "**Note:**\n",
    "- Your `fit` method prints the runtime per epoch, but in this task you should record the total runtime over training and prediction. (in seconds). To do this, it may make sense to use the time module to record the total time in the notebook cell below.\n",
    "- You should be running this on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP THIS SEED\n",
    "tf.random.set_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4f. Questions\n",
    "\n",
    "**Question 2:** What do the plots suggest to you about the relationship between batch size and accuracy? Please be specific,citing evidence from your plots.\n",
    "\n",
    "**Question 3:** Do you find this relationship surprising? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
