{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saad Khan and Nithun Selva**\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "Project 1: Deep Neural Networks \n",
    "\n",
    "#### Week 2: Training deeper networks with blocks\n",
    "\n",
    "The focus this week is on block design organizing deeper neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Building deeper neural networks with blocks\n",
    "\n",
    "In the quest to classify CIFAR-10 images with the highest accuracy possible, we would like to build a neural network that is deeper than VGG4 and has a greater capacity to learn more complex, nonlinear patterns in the images. Let's focus on designing a slightly deeper network than VGG4 that we will call VGG6 that has the following architecture:\n",
    "\n",
    "Conv2D → Conv2D → MaxPool2D → **Conv2D → Conv2D → MaxPool2D** → Flatten → *Dense → Dropout* → Dense\n",
    "\n",
    "Notice how the bold set of `Conv2D`/`MaxPool2D` layers are repeats of the layers to their left. It turns out, it may be beneficial to replicate the `Dense`/`Dropout` layers (italicized) toward the end of the network multiple times as well in deeper versions.\n",
    "\n",
    "Review your code for assembling `VGG4`. Building `VGG6` would require some copy-pasting of layer creation code. Imagine building even deeper versions with even more layers (e.g. `VGG9`) — this copy-paste process would get tedious, unwieldy, and potentially be error prone the bigger the network gets!\n",
    "\n",
    "For this reason, modern deep neural networks are often built using **blocks**: sequences of layers that repeat over and over again as you get farther into the network. For example, imagine replacing the layers **Conv2D → Conv2D → MaxPool2D** with a SINGLE new object that represents performing that sequence of those 3 layers. If we also do this for the `Dense`/`Dropout` layers, the architecture would look like:\n",
    "\n",
    "VGGConvBlock_0 → **VGGConvBlock_1** → Flatten → *VGGDenseBlock_0* → Dense\n",
    "\n",
    "Much simpler, more manageable, and easier to scale up to deeper nets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Build and test `VGG` blocks\n",
    "\n",
    "The file `block.py` contains both the `Block` class and the `VGGConvBlock` and `VGGDenseBlock` classes referenced above. The `Block` class is the parent class to all `Block` classes (*both ones you write this week and for the rest of the semester!*) and is designed to work with `DeepNetwork`. Just like `DeepNetwork`, it contains all the \"boilerplate\" code that needs to be written for ANY block.\n",
    "\n",
    "Aside from the constructor, I am providing you with the `Block` class fully implemented :) You only need to write code that assembles the layers that belong to a block and specify how the forward pass thru them is done. Blocks can be mixed-and-matched and interspersed with regular layers! Nice!\n",
    "\n",
    "Implement and test the following classes and methods.\n",
    "\n",
    "**Block:**\n",
    "- constructor.\n",
    "\n",
    "**VGGConvBlock:**\n",
    "- constructor: What layers belong to a `VGGConvBlock` block?\n",
    "- `__call__`: How do we perform the forward pass thru the block?\n",
    "\n",
    "**VGGDenseBlock:**\n",
    "- constructor: What layers belong to a `VGGDenseBlock` block?\n",
    "- `__call__`: How do we perform the forward pass thru the block?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from block import VGGConvBlock, VGGDenseBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGGConvBlock` part 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestBlock:\n",
      "\tMaxPool2D layer output(TestBlock/maxpool) shape: [1, 2, 2, 5]\n",
      "\tConv2D layer output(TestBlock/conv_1) shape: [1, 4, 4, 5]\n",
      "\tConv2D layer output(TestBlock/conv_0) shape: [1, 4, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "conv_block = VGGConvBlock('TestBlock', units=5, prev_layer_or_block=None, wt_scale=1e-1)\n",
    "conv_block(x_test_1)\n",
    "print(conv_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestBlock:\n",
    "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 5]\n",
    "\tConv2D layer output(TestBlock/conv1) shape: [1, 4, 4, 5]\n",
    "\tConv2D layer output(TestBlock/conv0) shape: [1, 4, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[[[0.372 0.487 0.071 0.158 0.   ]\n",
      "   [0.156 0.412 0.175 0.116 0.019]]\n",
      "\n",
      "  [[0.51  0.548 0.085 0.299 0.   ]\n",
      "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.25  0.551 0.    0.321 0.022]\n",
      "   [0.461 0.47  0.116 0.132 0.044]]\n",
      "\n",
      "  [[0.37  0.546 0.003 0.221 0.009]\n",
      "   [0.37  0.486 0.123 0.054 0.   ]]]]\n",
      "and they should be:\n",
      "[[[[0.372 0.487 0.071 0.158 0.   ]\n",
      "   [0.156 0.412 0.175 0.116 0.019]]\n",
      "\n",
      "  [[0.51  0.548 0.085 0.299 0.   ]\n",
      "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
      "\n",
      "\n",
      " [[[0.25  0.551 0.    0.321 0.022]\n",
      "   [0.461 0.47  0.116 0.132 0.044]]\n",
      "\n",
      "  [[0.37  0.546 0.003 0.221 0.009]\n",
      "   [0.37  0.486 0.123 0.054 0.   ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(2, 4, 4, 3))\n",
    "acts = conv_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.372 0.487 0.071 0.158 0.   ]\n",
    "   [0.156 0.412 0.175 0.116 0.019]]\n",
    "\n",
    "  [[0.51  0.548 0.085 0.299 0.   ]\n",
    "   [0.375 0.327 0.169 0.209 0.   ]]]\n",
    "\n",
    "\n",
    " [[[0.25  0.551 0.    0.321 0.022]\n",
    "   [0.461 0.47  0.116 0.132 0.044]]\n",
    "\n",
    "  [[0.37  0.546 0.003 0.221 0.009]\n",
    "   [0.37  0.486 0.123 0.054 0.   ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGGConvBlock` part 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestBlock:\n",
      "\tDropout layer output(TestBlock/dropout) shape: [1, 2, 2, 7]\n",
      "\tMaxPool2D layer output(TestBlock/maxpool) shape: [1, 2, 2, 7]\n",
      "\tConv2D layer output(TestBlock/conv_1) shape: [1, 4, 4, 7]\n",
      "\tConv2D layer output(TestBlock/conv_0) shape: [1, 4, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "conv_block = VGGConvBlock('TestBlock', units=7, prev_layer_or_block=None, dropout=True)\n",
    "conv_block(x_test_1)\n",
    "print(conv_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestBlock:\n",
    "\tDropout layer output(TestBlock/dropout) shape: [1, 2, 2, 7]\n",
    "\tMaxPool2D layer output(TestBlock/maxpool2) shape: [1, 2, 2, 7]\n",
    "\tConv2D layer output(TestBlock/conv1) shape: [1, 4, 4, 7]\n",
    "\tConv2D layer output(TestBlock/conv0) shape: [1, 4, 4, 7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]\n",
      "and they should be:\n",
      "[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
      "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(1, 4, 4, 3))\n",
    "acts = conv_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
    "   [0.002 0.002 0.    0.    0.    0.    0.   ]]\n",
    "\n",
    "  [[0.002 0.002 0.    0.    0.    0.    0.   ]\n",
    "   [0.002 0.002 0.    0.    0.    0.    0.   ]]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGGDenseBlock` part 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestDenseBlock:\n",
      "\tDropout layer output(TestDenseBlock/dropout_0) shape: [1, 2]\n",
      "\tDense layer output(TestDenseBlock/dense_0) shape: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 6))\n",
    "dense_block = VGGDenseBlock('TestDenseBlock', units=(2,), prev_layer_or_block=None, wt_scale=1e-1)\n",
    "dense_block(x_test_1)\n",
    "print(dense_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestDenseBlock:\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 2]\n",
    "\tDense layer output(TestDenseBlock/dense0) shape: [1, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[0.   0.  ]\n",
      " [0.   0.  ]\n",
      " [0.   0.07]]\n",
      "and they should be:\n",
      "[[0.   0.  ]\n",
      " [0.   0.  ]\n",
      " [0.   0.07]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(3, 6))\n",
    "acts = dense_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.   0.  ]\n",
    " [0.   0.  ]\n",
    " [0.   0.07]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGGDenseBlock` part 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestDenseBlock:\n",
      "\tDropout layer output(TestDenseBlock/dropout_1) shape: [1, 5]\n",
      "\tDense layer output(TestDenseBlock/dense_1) shape: [1, 5]\n",
      "\tDropout layer output(TestDenseBlock/dropout_0) shape: [1, 4]\n",
      "\tDense layer output(TestDenseBlock/dense_0) shape: [1, 4]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_1 = tf.random.normal(shape=(1, 7))\n",
    "dense_block = VGGDenseBlock('TestDenseBlock', units=(4,5), prev_layer_or_block=None, num_dense_blocks=2)\n",
    "dense_block(x_test_1)\n",
    "print(dense_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print (naming might be different):\n",
    "\n",
    "```\n",
    "TestDenseBlock:\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 5]\n",
    "\tDense layer output(TestDenseBlock/dense1) shape: [1, 5]\n",
    "\tDropout layer output(TestDenseBlock/dropout) shape: [1, 4]\n",
    "\tDense layer output(TestDenseBlock/dense0) shape: [1, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your block net_acts are\n",
      "[[0.002 0.002 0.    0.    0.   ]\n",
      " [0.002 0.002 0.    0.    0.   ]]\n",
      "and they should be:\n",
      "[[0.002 0.002 0.    0.    0.   ]\n",
      " [0.002 0.002 0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "x_test_2 = tf.random.normal(shape=(2, 7))\n",
    "acts = dense_block(x_test_2)\n",
    "print(f'Your block net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.002 0.002 0.    0.    0.   ]\n",
    " [0.002 0.002 0.    0.    0.   ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Build `VGG6`\n",
    "\n",
    "Now that you have both types of VGG blocks implemented and tested, make us of them to write the `VGG6` constructor and `__call__` methods in `vgg_nets.py`. This should be a quick process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg_nets import VGG6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `VGG6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 5]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout_0) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(Flatten) shape: [1, 512]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool) shape: [1, 2, 2, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 4, 4, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 4, 4, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool) shape: [1, 4, 4, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 8, 8, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 8, 8, 64]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_vgg6_0 = VGG6(C=5, input_feats_shape=(8, 8, 3))\n",
    "test_vgg6_0.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should print something like (*layer/block names may be different and that's ok*):\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 5]\n",
    "DenseBlock1:\n",
    "\tDropout layer output(DenseBlock1/dropout) shape: [1, 256]\n",
    "\tDense layer output(DenseBlock1/dense0) shape: [1, 256]\n",
    "Flatten layer output(flat) shape: [1, 512]\n",
    "ConvBlock2:\n",
    "\tMaxPool2D layer output(ConvBlock2/maxpool2) shape: [1, 2, 2, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv1) shape: [1, 4, 4, 128]\n",
    "\tConv2D layer output(ConvBlock2/conv0) shape: [1, 4, 4, 128]\n",
    "ConvBlock1:\n",
    "\tMaxPool2D layer output(ConvBlock1/maxpool2) shape: [1, 4, 4, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv1) shape: [1, 8, 8, 64]\n",
    "\tConv2D layer output(ConvBlock1/conv0) shape: [1, 8, 8, 64]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your VGG6 output layer net_acts are\n",
      "[[0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.999 0.    0.001]\n",
      " [0.    0.    0.652 0.003 0.345]]\n",
      "and they should be:\n",
      "[[0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.999 0.    0.001]\n",
      " [0.    0.    0.652 0.003 0.345]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "x_test_3 = tf.random.normal(shape=(6, 8, 8, 3))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "test_vgg6 = VGG6(C=5, input_feats_shape=(8, 8, 3), wt_scale=1e-1)\n",
    "acts = test_vgg6(x_test_3)\n",
    "print(f'Your VGG6 output layer net_acts are\\n{acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    1.    0.    0.   ]\n",
    " [0.    0.    0.999 0.    0.001]\n",
    " [0.    0.    0.652 0.003 0.345]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Train `VGG6` on CIFAR-10 with the default learning rate\n",
    "\n",
    "In the cells below:\n",
    "1. Load in the CIFAR-10 dataset.\n",
    "2. Train for `25` epochs with default lr and other default hyperparameters. Your initial training and val losses should be 2.30 and should hold steady.\n",
    "3. Print out the final test accuracy.\n",
    "\n",
    "#### Important notes\n",
    "\n",
    "#### 1. Running on CoCalc and GPU\n",
    "\n",
    "You should do this training session (and all subsequent \"real\" training sessions this semester on the GPU in CoCalc). Training at this point on your CPU is basically infeasible (*feel free to try it!*).\n",
    "\n",
    "#### 2. JIT compiling the train and test steps\n",
    "\n",
    "While training VGG6 on the GPU should take ~15 secs per epoch, which is not too bad, soon deeper networks and larger datasets will make the training too slow for us (*even on the GPU!*). To speed things up considerably now and going forward, use the process we discussed in class to decorate `train_step` and `test_step` with `@tf.function(jit_compile=True)`. The 1st epoch might be a little slow, but subsequent epochs should now fly by.\n",
    "\n",
    "**Note:**\n",
    "- If you have have trouble just-in-time (JIT) compiling the train and test steps, you should be able to decorate with `@tf.function` to statically compile the network (non-JIT). This may be slower than JIT compiling the network, but should still be faster than no compilation. **If JIT compiling does not work, please seek help. JIT compiling on CoCalc will be very helpful going forward.**\n",
    "- If you are training locally on macOS, JIT compiling will not work, but falling back to `@tf.function` should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, classnames = get_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 10]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout_0) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(Flatten) shape: [1, 8192]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 16, 16, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0972\n",
      "Epoch 1 completed in 15.93 seconds.\n",
      "Epoch 2/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 2 completed in 13.04 seconds.\n",
      "Epoch 3/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0958\n",
      "Epoch 3 completed in 12.93 seconds.\n",
      "Epoch 4/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 4 completed in 12.93 seconds.\n",
      "Epoch 5/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0952\n",
      "Epoch 5 completed in 12.96 seconds.\n",
      "Epoch 6/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0958\n",
      "Epoch 6 completed in 12.88 seconds.\n",
      "Epoch 7/25 - Train Loss: 2.3027, Val Loss: 2.3029, Val Acc: 0.0952\n",
      "Epoch 7 completed in 12.99 seconds.\n",
      "Epoch 8/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 8 completed in 12.93 seconds.\n",
      "Epoch 9/25 - Train Loss: 2.3027, Val Loss: 2.3029, Val Acc: 0.0952\n",
      "Epoch 9 completed in 12.87 seconds.\n",
      "Epoch 10/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0958\n",
      "Epoch 10 completed in 12.93 seconds.\n",
      "Epoch 11/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0976\n",
      "Epoch 11 completed in 12.87 seconds.\n",
      "Epoch 12/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0952\n",
      "Epoch 12 completed in 12.99 seconds.\n",
      "Epoch 13/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0952\n",
      "Epoch 13 completed in 13.83 seconds.\n",
      "Epoch 14/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0972\n",
      "Epoch 14 completed in 13.78 seconds.\n",
      "Epoch 15/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0972\n",
      "Epoch 15 completed in 13.07 seconds.\n",
      "Epoch 16/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 16 completed in 13.18 seconds.\n",
      "Epoch 17/25 - Train Loss: 2.3027, Val Loss: 2.3029, Val Acc: 0.0952\n",
      "Epoch 17 completed in 13.18 seconds.\n",
      "Epoch 18/25 - Train Loss: 2.3027, Val Loss: 2.3029, Val Acc: 0.0952\n",
      "Epoch 18 completed in 13.17 seconds.\n",
      "Epoch 19/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0952\n",
      "Epoch 19 completed in 12.91 seconds.\n",
      "Epoch 20/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0972\n",
      "Epoch 20 completed in 12.96 seconds.\n",
      "Epoch 21/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0958\n",
      "Epoch 21 completed in 13.00 seconds.\n",
      "Epoch 22/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 22 completed in 13.13 seconds.\n",
      "Epoch 23/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0958\n",
      "Epoch 23 completed in 13.27 seconds.\n",
      "Epoch 24/25 - Train Loss: 2.3027, Val Loss: 2.3028, Val Acc: 0.0972\n",
      "Epoch 24 completed in 13.27 seconds.\n",
      "Epoch 25/25 - Train Loss: 2.3027, Val Loss: 2.3027, Val Acc: 0.0972\n",
      "Epoch 25 completed in 13.05 seconds.\n",
      "Finished training after 25 epochs!\n",
      "Test Accuracy: 0.1001\n"
     ]
    }
   ],
   "source": [
    "# KEEP ME\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Train the model\n",
    "model = VGG6(C=10, input_feats_shape=(32, 32, 3))\n",
    "model.compile(optimizer='adam', loss='cross_entropy')\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(x_train, y_train, x_val, y_val, max_epochs=25)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "model.set_layer_training_mode(False)\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Train `VGG6` on CIFAR-10 with a smaller learning rate\n",
    "\n",
    "In the cells below, repeat what you did in the previous subtask, but this time change the learning rate to `1e-5`. You should get a very different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 10]\n",
      "DenseBlock1:\n",
      "\tDropout layer output(DenseBlock1/dropout_0) shape: [1, 256]\n",
      "\tDense layer output(DenseBlock1/dense_0) shape: [1, 256]\n",
      "Flatten layer output(Flatten) shape: [1, 8192]\n",
      "ConvBlock2:\n",
      "\tMaxPool2D layer output(ConvBlock2/maxpool) shape: [1, 8, 8, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_1) shape: [1, 16, 16, 128]\n",
      "\tConv2D layer output(ConvBlock2/conv_0) shape: [1, 16, 16, 128]\n",
      "ConvBlock1:\n",
      "\tMaxPool2D layer output(ConvBlock1/maxpool) shape: [1, 16, 16, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_1) shape: [1, 32, 32, 64]\n",
      "\tConv2D layer output(ConvBlock1/conv_0) shape: [1, 32, 32, 64]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 1 completed in 15.80 seconds.\n",
      "Epoch 2/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 2 completed in 12.99 seconds.\n",
      "Epoch 3/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 3 completed in 12.87 seconds.\n",
      "Epoch 4/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 4 completed in 13.24 seconds.\n",
      "Epoch 5/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 5 completed in 13.06 seconds.\n",
      "Epoch 6/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.1040\n",
      "Epoch 6 completed in 13.41 seconds.\n",
      "Epoch 7/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 7 completed in 13.31 seconds.\n",
      "Epoch 8/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 8 completed in 13.26 seconds.\n",
      "Epoch 9/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 9 completed in 13.27 seconds.\n",
      "Epoch 10/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 10 completed in 13.26 seconds.\n",
      "Epoch 11/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 11 completed in 13.26 seconds.\n",
      "Epoch 12/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 12 completed in 12.91 seconds.\n",
      "Epoch 13/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 13 completed in 12.92 seconds.\n",
      "Epoch 14/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 14 completed in 13.39 seconds.\n",
      "Epoch 15/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 15 completed in 13.15 seconds.\n",
      "Epoch 16/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 16 completed in 12.91 seconds.\n",
      "Epoch 17/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 17 completed in 13.14 seconds.\n",
      "Epoch 18/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 18 completed in 13.06 seconds.\n",
      "Epoch 19/25 - Train Loss: 2.3026, Val Loss: 2.3026, Val Acc: 0.0972\n",
      "Epoch 19 completed in 13.39 seconds.\n",
      "Epoch 20/25 - Train Loss: 2.2901, Val Loss: 2.1771, Val Acc: 0.1727\n",
      "Epoch 20 completed in 23.68 seconds.\n",
      "Epoch 21/25 - Train Loss: 2.0365, Val Loss: 1.9715, Val Acc: 0.2897\n",
      "Epoch 21 completed in 14.31 seconds.\n",
      "Epoch 22/25 - Train Loss: 1.9506, Val Loss: 1.9243, Val Acc: 0.3065\n",
      "Epoch 22 completed in 12.94 seconds.\n",
      "Epoch 23/25 - Train Loss: 1.9084, Val Loss: 1.8887, Val Acc: 0.3291\n",
      "Epoch 23 completed in 12.95 seconds.\n",
      "Epoch 24/25 - Train Loss: 1.8755, Val Loss: 1.8508, Val Acc: 0.3385\n",
      "Epoch 24 completed in 12.89 seconds.\n",
      "Epoch 25/25 - Train Loss: 1.8429, Val Loss: 1.8183, Val Acc: 0.3478\n",
      "Epoch 25 completed in 12.94 seconds.\n",
      "Finished training after 25 epochs!\n",
      "Test Accuracy: 0.3578\n"
     ]
    }
   ],
   "source": [
    "# KEEP ME\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Train the model\n",
    "model = VGG6(C=10, input_feats_shape=(32, 32, 3))\n",
    "model.compile(optimizer='adam', loss='cross_entropy', lr=1e-5)\n",
    "train_loss_hist, val_loss_hist, val_acc_hist, e = model.fit(x_train, y_train, x_val, y_val, max_epochs=25)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "model.set_layer_training_mode(False)\n",
    "test_acc, test_loss = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5e. Questions\n",
    "\n",
    "**Question 4:** How does the modified learning rate compare with the default? Why do you think you observed what you did for VGG6 and not VGG4 trained on CIFAR-10 with the default lr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Early stopping and He/Kaiming initialization\n",
    "\n",
    "The experiment that you just ran illuminates two major issues with our training workflow:\n",
    "1. Cutting off training while the net is learning after waiting a long time at some prespecified number of epochs is frustrating. It would be nice to not have to manually set the number of training epochs as long as the net is making progress.\n",
    "2. Picking the correct lr that could make or break training is frustrating. It would be nice to have the net work well to a wide range of lr choices and number of layers.\n",
    "\n",
    "In this section, we will introduce the following techniques to combat these respective issues:\n",
    "1. Early stopping.\n",
    "2. He/Kaiming weight initialization (*next week*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import DeepNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. Implement early stopping\n",
    "\n",
    "Implement the `early_stopping` method in `DeepNetwork` to determine the appropriate conditions to stop during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `early_stopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping Test 1 (patience_1=5):\n",
      " Stopped after 5 iterations (should be 5 iterations).\n",
      " Recent loss history is [1.0, 2.0, 3.0, 4.0, 5.0] and should be [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "\n",
      "Early stopping Test 2 (patience_2=3):\n",
      " Stopped after 6 iterations (should be 6 iterations).\n",
      " Recent loss history is [np.float32(0.29193902), np.float32(0.64250207), np.float32(0.9757855)] and should be [0.29193902, 0.64250207, 0.9757855]\n",
      "\n",
      "Early stopping Test 3 (patience_3=6):\n",
      " Stopped after 9 iterations (should be 9 iterations).\n",
      " Recent loss history is\n",
      " [np.float32(0.29193902), np.float32(0.64250207), np.float32(0.9757855), np.float32(0.43509948), np.float32(0.6601019), np.float32(0.60489583)]\n",
      " and should be\n",
      " [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn = DeepNetwork((1,), 0.)\n",
    "\n",
    "# Test 1\n",
    "patience_1 = 5\n",
    "es_lost_hist_1 = []\n",
    "for iter in range(10):\n",
    "    curr_loss = float(iter)\n",
    "    es_lost_hist_1, stop = dn.early_stopping(es_lost_hist_1, curr_loss, patience=patience_1)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 1 ({patience_1=}):\\n Stopped after {iter} iterations (should be 5 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_1} and should be [1.0, 2.0, 3.0, 4.0, 5.0]')\n",
    "print()\n",
    "\n",
    "# Test 2\n",
    "tf.random.set_seed(1)\n",
    "patience_2 = 3\n",
    "es_lost_hist_2 = []\n",
    "test_2_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_2_loss_vals[iter]\n",
    "    es_lost_hist_2, stop = dn.early_stopping(es_lost_hist_2, curr_loss, patience=patience_2)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 2 ({patience_2=}):\\n Stopped after {iter} iterations (should be 6 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_2} and should be [0.29193902, 0.64250207, 0.9757855]')\n",
    "print()\n",
    "\n",
    "# Test 3\n",
    "tf.random.set_seed(1)\n",
    "patience_3 = 6\n",
    "es_lost_hist_3 = []\n",
    "test_3_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_3_loss_vals[iter]\n",
    "    es_lost_hist_3, stop = dn.early_stopping(es_lost_hist_3, curr_loss, patience=patience_3)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 3 ({patience_3=}):\\n Stopped after {iter} iterations (should be 9 iterations).')\n",
    "print(f' Recent loss history is\\n {es_lost_hist_3}\\n and should be')\n",
    "print(' [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Integrate early stopping into training\n",
    "\n",
    "Modify your `fit` function to support early stopping. Here are the changes to make:\n",
    "\n",
    "1. Before the training loop create an empty list to record the rolling list of recent validation loss values within the patience window of epochs.\n",
    "2. Each time the validation loss is computed, update and check the early stopping conditions. If the conditions are met, end the training early before `max_epochs` epochs is reached.\n",
    "3. Make sure you are returning as the 4th return argument the number of epochs before training ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `fit` with early stopping\n",
    "\n",
    "The following test should end:\n",
    "- in about 10 secs.\n",
    "- after 300 epochs.\n",
    "- with final training loss of 0.04, Val loss of 0.06, Val acc of 96.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(TestDense) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1 completed in 0.36 seconds.\n",
      "Epoch 2 completed in 0.02 seconds.\n",
      "Epoch 3 completed in 0.02 seconds.\n",
      "Epoch 4 completed in 0.02 seconds.\n",
      "Epoch 5 completed in 0.02 seconds.\n",
      "Epoch 6 completed in 0.02 seconds.\n",
      "Epoch 7 completed in 0.02 seconds.\n",
      "Epoch 8 completed in 0.02 seconds.\n",
      "Epoch 9 completed in 0.02 seconds.\n",
      "Epoch 10 completed in 0.02 seconds.\n",
      "Epoch 11 completed in 0.02 seconds.\n",
      "Epoch 12 completed in 0.02 seconds.\n",
      "Epoch 13 completed in 0.02 seconds.\n",
      "Epoch 14 completed in 0.02 seconds.\n",
      "Epoch 15 completed in 0.02 seconds.\n",
      "Epoch 16 completed in 0.02 seconds.\n",
      "Epoch 17 completed in 0.02 seconds.\n",
      "Epoch 18 completed in 0.02 seconds.\n",
      "Epoch 19 completed in 0.02 seconds.\n",
      "Epoch 20 completed in 0.02 seconds.\n",
      "Epoch 21 completed in 0.02 seconds.\n",
      "Epoch 22 completed in 0.02 seconds.\n",
      "Epoch 23 completed in 0.02 seconds.\n",
      "Epoch 24 completed in 0.02 seconds.\n",
      "Epoch 25 completed in 0.02 seconds.\n",
      "Epoch 26 completed in 0.02 seconds.\n",
      "Epoch 27 completed in 0.02 seconds.\n",
      "Epoch 28 completed in 0.02 seconds.\n",
      "Epoch 29 completed in 0.02 seconds.\n",
      "Epoch 30 completed in 0.02 seconds.\n",
      "Epoch 31 completed in 0.02 seconds.\n",
      "Epoch 32 completed in 0.02 seconds.\n",
      "Epoch 33 completed in 0.02 seconds.\n",
      "Epoch 34 completed in 0.02 seconds.\n",
      "Epoch 35 completed in 0.02 seconds.\n",
      "Epoch 36 completed in 0.02 seconds.\n",
      "Epoch 37 completed in 0.02 seconds.\n",
      "Epoch 38 completed in 0.02 seconds.\n",
      "Epoch 39 completed in 0.02 seconds.\n",
      "Epoch 40 completed in 0.02 seconds.\n",
      "Epoch 41 completed in 0.02 seconds.\n",
      "Epoch 42 completed in 0.02 seconds.\n",
      "Epoch 43 completed in 0.02 seconds.\n",
      "Epoch 44 completed in 0.02 seconds.\n",
      "Epoch 45 completed in 0.02 seconds.\n",
      "Epoch 46 completed in 0.02 seconds.\n",
      "Epoch 47 completed in 0.02 seconds.\n",
      "Epoch 48 completed in 0.02 seconds.\n",
      "Epoch 49 completed in 0.02 seconds.\n",
      "Epoch 50 completed in 0.02 seconds.\n",
      "Epoch 51 completed in 0.02 seconds.\n",
      "Epoch 52 completed in 0.02 seconds.\n",
      "Epoch 53 completed in 0.02 seconds.\n",
      "Epoch 54 completed in 0.02 seconds.\n",
      "Epoch 55 completed in 0.02 seconds.\n",
      "Epoch 56 completed in 0.02 seconds.\n",
      "Epoch 57 completed in 0.02 seconds.\n",
      "Epoch 58 completed in 0.02 seconds.\n",
      "Epoch 59 completed in 0.02 seconds.\n",
      "Epoch 60 completed in 0.02 seconds.\n",
      "Epoch 61 completed in 0.02 seconds.\n",
      "Epoch 62 completed in 0.02 seconds.\n",
      "Epoch 63 completed in 0.02 seconds.\n",
      "Epoch 64 completed in 0.02 seconds.\n",
      "Epoch 65 completed in 0.02 seconds.\n",
      "Epoch 66 completed in 0.02 seconds.\n",
      "Epoch 67 completed in 0.02 seconds.\n",
      "Epoch 68 completed in 0.02 seconds.\n",
      "Epoch 69 completed in 0.02 seconds.\n",
      "Epoch 70 completed in 0.02 seconds.\n",
      "Epoch 71 completed in 0.02 seconds.\n",
      "Epoch 72 completed in 0.02 seconds.\n",
      "Epoch 73 completed in 0.02 seconds.\n",
      "Epoch 74 completed in 0.02 seconds.\n",
      "Epoch 75 completed in 0.02 seconds.\n",
      "Epoch 76 completed in 0.02 seconds.\n",
      "Epoch 77 completed in 0.02 seconds.\n",
      "Epoch 78 completed in 0.02 seconds.\n",
      "Epoch 79 completed in 0.02 seconds.\n",
      "Epoch 80 completed in 0.02 seconds.\n",
      "Epoch 81 completed in 0.02 seconds.\n",
      "Epoch 82 completed in 0.02 seconds.\n",
      "Epoch 83 completed in 0.02 seconds.\n",
      "Epoch 84 completed in 0.02 seconds.\n",
      "Epoch 85 completed in 0.02 seconds.\n",
      "Epoch 86 completed in 0.02 seconds.\n",
      "Epoch 87 completed in 0.02 seconds.\n",
      "Epoch 88 completed in 0.02 seconds.\n",
      "Epoch 89 completed in 0.02 seconds.\n",
      "Epoch 90 completed in 0.02 seconds.\n",
      "Epoch 91 completed in 0.02 seconds.\n",
      "Epoch 92 completed in 0.02 seconds.\n",
      "Epoch 93 completed in 0.02 seconds.\n",
      "Epoch 94 completed in 0.02 seconds.\n",
      "Epoch 95 completed in 0.02 seconds.\n",
      "Epoch 96 completed in 0.02 seconds.\n",
      "Epoch 97 completed in 0.02 seconds.\n",
      "Epoch 98 completed in 0.02 seconds.\n",
      "Epoch 99 completed in 0.02 seconds.\n",
      "Epoch 100/5000 - Train Loss: 0.0782, Val Loss: 0.1091, Val Acc: 0.9592\n",
      "Epoch 100 completed in 0.11 seconds.\n",
      "Epoch 101 completed in 0.02 seconds.\n",
      "Epoch 102 completed in 0.02 seconds.\n",
      "Epoch 103 completed in 0.02 seconds.\n",
      "Epoch 104 completed in 0.02 seconds.\n",
      "Epoch 105 completed in 0.02 seconds.\n",
      "Epoch 106 completed in 0.02 seconds.\n",
      "Epoch 107 completed in 0.02 seconds.\n",
      "Epoch 108 completed in 0.02 seconds.\n",
      "Epoch 109 completed in 0.02 seconds.\n",
      "Epoch 110 completed in 0.02 seconds.\n",
      "Epoch 111 completed in 0.02 seconds.\n",
      "Epoch 112 completed in 0.02 seconds.\n",
      "Epoch 113 completed in 0.02 seconds.\n",
      "Epoch 114 completed in 0.02 seconds.\n",
      "Epoch 115 completed in 0.02 seconds.\n",
      "Epoch 116 completed in 0.02 seconds.\n",
      "Epoch 117 completed in 0.02 seconds.\n",
      "Epoch 118 completed in 0.02 seconds.\n",
      "Epoch 119 completed in 0.02 seconds.\n",
      "Epoch 120 completed in 0.02 seconds.\n",
      "Epoch 121 completed in 0.02 seconds.\n",
      "Epoch 122 completed in 0.02 seconds.\n",
      "Epoch 123 completed in 0.02 seconds.\n",
      "Epoch 124 completed in 0.02 seconds.\n",
      "Epoch 125 completed in 0.02 seconds.\n",
      "Epoch 126 completed in 0.02 seconds.\n",
      "Epoch 127 completed in 0.02 seconds.\n",
      "Epoch 128 completed in 0.02 seconds.\n",
      "Epoch 129 completed in 0.02 seconds.\n",
      "Epoch 130 completed in 0.02 seconds.\n",
      "Epoch 131 completed in 0.02 seconds.\n",
      "Epoch 132 completed in 0.02 seconds.\n",
      "Epoch 133 completed in 0.02 seconds.\n",
      "Epoch 134 completed in 0.02 seconds.\n",
      "Epoch 135 completed in 0.02 seconds.\n",
      "Epoch 136 completed in 0.02 seconds.\n",
      "Epoch 137 completed in 0.02 seconds.\n",
      "Epoch 138 completed in 0.02 seconds.\n",
      "Epoch 139 completed in 0.02 seconds.\n",
      "Epoch 140 completed in 0.02 seconds.\n",
      "Epoch 141 completed in 0.02 seconds.\n",
      "Epoch 142 completed in 0.02 seconds.\n",
      "Epoch 143 completed in 0.02 seconds.\n",
      "Epoch 144 completed in 0.02 seconds.\n",
      "Epoch 145 completed in 0.02 seconds.\n",
      "Epoch 146 completed in 0.02 seconds.\n",
      "Epoch 147 completed in 0.02 seconds.\n",
      "Epoch 148 completed in 0.02 seconds.\n",
      "Epoch 149 completed in 0.02 seconds.\n",
      "Epoch 150 completed in 0.02 seconds.\n",
      "Epoch 151 completed in 0.02 seconds.\n",
      "Epoch 152 completed in 0.02 seconds.\n",
      "Epoch 153 completed in 0.02 seconds.\n",
      "Epoch 154 completed in 0.02 seconds.\n",
      "Epoch 155 completed in 0.02 seconds.\n",
      "Epoch 156 completed in 0.02 seconds.\n",
      "Epoch 157 completed in 0.02 seconds.\n",
      "Epoch 158 completed in 0.02 seconds.\n",
      "Epoch 159 completed in 0.02 seconds.\n",
      "Epoch 160 completed in 0.02 seconds.\n",
      "Epoch 161 completed in 0.02 seconds.\n",
      "Epoch 162 completed in 0.02 seconds.\n",
      "Epoch 163 completed in 0.02 seconds.\n",
      "Epoch 164 completed in 0.02 seconds.\n",
      "Epoch 165 completed in 0.02 seconds.\n",
      "Epoch 166 completed in 0.02 seconds.\n",
      "Epoch 167 completed in 0.02 seconds.\n",
      "Epoch 168 completed in 0.02 seconds.\n",
      "Epoch 169 completed in 0.02 seconds.\n",
      "Epoch 170 completed in 0.02 seconds.\n",
      "Epoch 171 completed in 0.02 seconds.\n",
      "Epoch 172 completed in 0.02 seconds.\n",
      "Epoch 173 completed in 0.02 seconds.\n",
      "Epoch 174 completed in 0.02 seconds.\n",
      "Epoch 175 completed in 0.02 seconds.\n",
      "Epoch 176 completed in 0.02 seconds.\n",
      "Epoch 177 completed in 0.02 seconds.\n",
      "Epoch 178 completed in 0.02 seconds.\n",
      "Epoch 179 completed in 0.02 seconds.\n",
      "Epoch 180 completed in 0.02 seconds.\n",
      "Epoch 181 completed in 0.02 seconds.\n",
      "Epoch 182 completed in 0.02 seconds.\n",
      "Epoch 183 completed in 0.02 seconds.\n",
      "Epoch 184 completed in 0.02 seconds.\n",
      "Epoch 185 completed in 0.02 seconds.\n",
      "Epoch 186 completed in 0.02 seconds.\n",
      "Epoch 187 completed in 0.02 seconds.\n",
      "Epoch 188 completed in 0.02 seconds.\n",
      "Epoch 189 completed in 0.02 seconds.\n",
      "Epoch 190 completed in 0.02 seconds.\n",
      "Epoch 191 completed in 0.02 seconds.\n",
      "Epoch 192 completed in 0.02 seconds.\n",
      "Epoch 193 completed in 0.02 seconds.\n",
      "Epoch 194 completed in 0.02 seconds.\n",
      "Epoch 195 completed in 0.02 seconds.\n",
      "Epoch 196 completed in 0.02 seconds.\n",
      "Epoch 197 completed in 0.02 seconds.\n",
      "Epoch 198 completed in 0.02 seconds.\n",
      "Epoch 199 completed in 0.02 seconds.\n",
      "Epoch 200/5000 - Train Loss: 0.0602, Val Loss: 0.0921, Val Acc: 0.9592\n",
      "Epoch 200 completed in 0.03 seconds.\n",
      "Epoch 201 completed in 0.02 seconds.\n",
      "Epoch 202 completed in 0.02 seconds.\n",
      "Epoch 203 completed in 0.02 seconds.\n",
      "Epoch 204 completed in 0.02 seconds.\n",
      "Epoch 205 completed in 0.02 seconds.\n",
      "Epoch 206 completed in 0.02 seconds.\n",
      "Epoch 207 completed in 0.02 seconds.\n",
      "Epoch 208 completed in 0.02 seconds.\n",
      "Epoch 209 completed in 0.02 seconds.\n",
      "Epoch 210 completed in 0.02 seconds.\n",
      "Epoch 211 completed in 0.02 seconds.\n",
      "Epoch 212 completed in 0.02 seconds.\n",
      "Epoch 213 completed in 0.02 seconds.\n",
      "Epoch 214 completed in 0.02 seconds.\n",
      "Epoch 215 completed in 0.02 seconds.\n",
      "Epoch 216 completed in 0.02 seconds.\n",
      "Epoch 217 completed in 0.02 seconds.\n",
      "Epoch 218 completed in 0.02 seconds.\n",
      "Epoch 219 completed in 0.02 seconds.\n",
      "Epoch 220 completed in 0.02 seconds.\n",
      "Epoch 221 completed in 0.02 seconds.\n",
      "Epoch 222 completed in 0.02 seconds.\n",
      "Epoch 223 completed in 0.02 seconds.\n",
      "Epoch 224 completed in 0.02 seconds.\n",
      "Epoch 225 completed in 0.02 seconds.\n",
      "Epoch 226 completed in 0.02 seconds.\n",
      "Epoch 227 completed in 0.02 seconds.\n",
      "Epoch 228 completed in 0.02 seconds.\n",
      "Epoch 229 completed in 0.02 seconds.\n",
      "Epoch 230 completed in 0.02 seconds.\n",
      "Epoch 231 completed in 0.02 seconds.\n",
      "Epoch 232 completed in 0.02 seconds.\n",
      "Epoch 233 completed in 0.02 seconds.\n",
      "Epoch 234 completed in 0.02 seconds.\n",
      "Epoch 235 completed in 0.02 seconds.\n",
      "Epoch 236 completed in 0.02 seconds.\n",
      "Epoch 237 completed in 0.02 seconds.\n",
      "Epoch 238 completed in 0.02 seconds.\n",
      "Epoch 239 completed in 0.02 seconds.\n",
      "Epoch 240 completed in 0.02 seconds.\n",
      "Epoch 241 completed in 0.02 seconds.\n",
      "Epoch 242 completed in 0.02 seconds.\n",
      "Epoch 243 completed in 0.02 seconds.\n",
      "Epoch 244 completed in 0.02 seconds.\n",
      "Epoch 245 completed in 0.02 seconds.\n",
      "Epoch 246 completed in 0.02 seconds.\n",
      "Epoch 247 completed in 0.02 seconds.\n",
      "Epoch 248 completed in 0.02 seconds.\n",
      "Epoch 249 completed in 0.02 seconds.\n",
      "Epoch 250 completed in 0.02 seconds.\n",
      "Epoch 251 completed in 0.03 seconds.\n",
      "Epoch 252 completed in 0.02 seconds.\n",
      "Epoch 253 completed in 0.02 seconds.\n",
      "Epoch 254 completed in 0.02 seconds.\n",
      "Epoch 255 completed in 0.02 seconds.\n",
      "Epoch 256 completed in 0.02 seconds.\n",
      "Epoch 257 completed in 0.02 seconds.\n",
      "Epoch 258 completed in 0.02 seconds.\n",
      "Epoch 259 completed in 0.02 seconds.\n",
      "Epoch 260 completed in 0.02 seconds.\n",
      "Epoch 261 completed in 0.02 seconds.\n",
      "Epoch 262 completed in 0.02 seconds.\n",
      "Epoch 263 completed in 0.02 seconds.\n",
      "Epoch 264 completed in 0.02 seconds.\n",
      "Epoch 265 completed in 0.02 seconds.\n",
      "Epoch 266 completed in 0.02 seconds.\n",
      "Epoch 267 completed in 0.02 seconds.\n",
      "Epoch 268 completed in 0.02 seconds.\n",
      "Epoch 269 completed in 0.02 seconds.\n",
      "Epoch 270 completed in 0.02 seconds.\n",
      "Epoch 271 completed in 0.02 seconds.\n",
      "Epoch 272 completed in 0.02 seconds.\n",
      "Epoch 273 completed in 0.02 seconds.\n",
      "Epoch 274 completed in 0.02 seconds.\n",
      "Epoch 275 completed in 0.02 seconds.\n",
      "Epoch 276 completed in 0.02 seconds.\n",
      "Epoch 277 completed in 0.02 seconds.\n",
      "Epoch 278 completed in 0.02 seconds.\n",
      "Epoch 279 completed in 0.02 seconds.\n",
      "Epoch 280 completed in 0.02 seconds.\n",
      "Epoch 281 completed in 0.02 seconds.\n",
      "Epoch 282 completed in 0.02 seconds.\n",
      "Epoch 283 completed in 0.02 seconds.\n",
      "Epoch 284 completed in 0.02 seconds.\n",
      "Epoch 285 completed in 0.02 seconds.\n",
      "Epoch 286 completed in 0.02 seconds.\n",
      "Epoch 287 completed in 0.02 seconds.\n",
      "Epoch 288 completed in 0.02 seconds.\n",
      "Epoch 289 completed in 0.02 seconds.\n",
      "Epoch 290 completed in 0.02 seconds.\n",
      "Epoch 291 completed in 0.02 seconds.\n",
      "Epoch 292 completed in 0.02 seconds.\n",
      "Epoch 293 completed in 0.02 seconds.\n",
      "Epoch 294 completed in 0.02 seconds.\n",
      "Epoch 295 completed in 0.02 seconds.\n",
      "Epoch 296 completed in 0.02 seconds.\n",
      "Epoch 297 completed in 0.02 seconds.\n",
      "Epoch 298 completed in 0.02 seconds.\n",
      "Epoch 299 completed in 0.02 seconds.\n",
      "Epoch 300/5000 - Train Loss: 0.0653, Val Loss: 0.1337, Val Acc: 0.9592\n",
      "Epoch 300 completed in 0.03 seconds.\n",
      "Epoch 301 completed in 0.02 seconds.\n",
      "Epoch 302 completed in 0.02 seconds.\n",
      "Epoch 303 completed in 0.02 seconds.\n",
      "Epoch 304 completed in 0.02 seconds.\n",
      "Epoch 305 completed in 0.02 seconds.\n",
      "Epoch 306 completed in 0.02 seconds.\n",
      "Epoch 307 completed in 0.02 seconds.\n",
      "Epoch 308 completed in 0.02 seconds.\n",
      "Epoch 309 completed in 0.02 seconds.\n",
      "Epoch 310 completed in 0.02 seconds.\n",
      "Epoch 311 completed in 0.02 seconds.\n",
      "Epoch 312 completed in 0.02 seconds.\n",
      "Epoch 313 completed in 0.02 seconds.\n",
      "Epoch 314 completed in 0.02 seconds.\n",
      "Epoch 315 completed in 0.02 seconds.\n",
      "Epoch 316 completed in 0.02 seconds.\n",
      "Epoch 317 completed in 0.02 seconds.\n",
      "Epoch 318 completed in 0.02 seconds.\n",
      "Epoch 319 completed in 0.02 seconds.\n",
      "Epoch 320 completed in 0.02 seconds.\n",
      "Epoch 321 completed in 0.02 seconds.\n",
      "Epoch 322 completed in 0.02 seconds.\n",
      "Epoch 323 completed in 0.02 seconds.\n",
      "Epoch 324 completed in 0.02 seconds.\n",
      "Epoch 325 completed in 0.02 seconds.\n",
      "Epoch 326 completed in 0.02 seconds.\n",
      "Epoch 327 completed in 0.02 seconds.\n",
      "Epoch 328 completed in 0.02 seconds.\n",
      "Epoch 329 completed in 0.02 seconds.\n",
      "Epoch 330 completed in 0.02 seconds.\n",
      "Epoch 331 completed in 0.02 seconds.\n",
      "Epoch 332 completed in 0.02 seconds.\n",
      "Epoch 333 completed in 0.02 seconds.\n",
      "Epoch 334 completed in 0.02 seconds.\n",
      "Epoch 335 completed in 0.02 seconds.\n",
      "Epoch 336 completed in 0.02 seconds.\n",
      "Epoch 337 completed in 0.02 seconds.\n",
      "Epoch 338 completed in 0.02 seconds.\n",
      "Epoch 339 completed in 0.02 seconds.\n",
      "Epoch 340 completed in 0.02 seconds.\n",
      "Epoch 341 completed in 0.02 seconds.\n",
      "Epoch 342 completed in 0.02 seconds.\n",
      "Epoch 343 completed in 0.02 seconds.\n",
      "Epoch 344 completed in 0.02 seconds.\n",
      "Epoch 345 completed in 0.02 seconds.\n",
      "Epoch 346 completed in 0.02 seconds.\n",
      "Epoch 347 completed in 0.02 seconds.\n",
      "Epoch 348 completed in 0.02 seconds.\n",
      "Epoch 349 completed in 0.02 seconds.\n",
      "Epoch 350 completed in 0.02 seconds.\n",
      "Epoch 351 completed in 0.02 seconds.\n",
      "Epoch 352 completed in 0.02 seconds.\n",
      "Epoch 353 completed in 0.02 seconds.\n",
      "Epoch 354 completed in 0.02 seconds.\n",
      "Epoch 355 completed in 0.02 seconds.\n",
      "Epoch 356 completed in 0.02 seconds.\n",
      "Epoch 357 completed in 0.02 seconds.\n",
      "Epoch 358 completed in 0.02 seconds.\n",
      "Epoch 359 completed in 0.02 seconds.\n",
      "Epoch 360 completed in 0.02 seconds.\n",
      "Epoch 361 completed in 0.02 seconds.\n",
      "Epoch 362 completed in 0.02 seconds.\n",
      "Epoch 363 completed in 0.02 seconds.\n",
      "Epoch 364 completed in 0.02 seconds.\n",
      "Epoch 365 completed in 0.02 seconds.\n",
      "Epoch 366 completed in 0.02 seconds.\n",
      "Epoch 367 completed in 0.02 seconds.\n",
      "Epoch 368 completed in 0.02 seconds.\n",
      "Epoch 369 completed in 0.02 seconds.\n",
      "Epoch 370 completed in 0.02 seconds.\n",
      "Epoch 371 completed in 0.02 seconds.\n",
      "Epoch 372 completed in 0.02 seconds.\n",
      "Epoch 373 completed in 0.02 seconds.\n",
      "Epoch 374 completed in 0.02 seconds.\n",
      "Epoch 375 completed in 0.02 seconds.\n",
      "Epoch 376 completed in 0.02 seconds.\n",
      "Epoch 377 completed in 0.02 seconds.\n",
      "Epoch 378 completed in 0.02 seconds.\n",
      "Epoch 379 completed in 0.02 seconds.\n",
      "Epoch 380 completed in 0.02 seconds.\n",
      "Epoch 381 completed in 0.02 seconds.\n",
      "Epoch 382 completed in 0.02 seconds.\n",
      "Epoch 383 completed in 0.02 seconds.\n",
      "Epoch 384 completed in 0.02 seconds.\n",
      "Epoch 385 completed in 0.02 seconds.\n",
      "Epoch 386 completed in 0.02 seconds.\n",
      "Epoch 387 completed in 0.02 seconds.\n",
      "Epoch 388 completed in 0.02 seconds.\n",
      "Epoch 389 completed in 0.02 seconds.\n",
      "Epoch 390 completed in 0.02 seconds.\n",
      "Epoch 391 completed in 0.02 seconds.\n",
      "Epoch 392 completed in 0.02 seconds.\n",
      "Epoch 393 completed in 0.02 seconds.\n",
      "Epoch 394 completed in 0.02 seconds.\n",
      "Epoch 395 completed in 0.02 seconds.\n",
      "Epoch 396 completed in 0.02 seconds.\n",
      "Epoch 397 completed in 0.02 seconds.\n",
      "Epoch 398 completed in 0.02 seconds.\n",
      "Epoch 399 completed in 0.02 seconds.\n",
      "Early stopping at epoch 400\n",
      "Finished training after 400 epochs!\n",
      "---------------------------------------------------------------------------\n",
      "Iris test ended after 400 epochs with final val loss/acc of 0.18/0.96\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quickly make a mock network for testing\n",
    "class SoftmaxNet(DeepNetwork):\n",
    "    def __init__(self, input_feats_shape, C, reg=0):\n",
    "        super().__init__(input_feats_shape, reg)\n",
    "        self.output_layer = Dense('TestDense', units=C, activation='softmax', prev_layer_or_block=None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Load in Iris train/validation sets\n",
    "train_samps = tf.constant(np.load('data/iris/iris_train_samps.npy'), dtype=tf.float32)\n",
    "train_labels = tf.constant(np.load('data/iris/iris_train_labels.npy'), dtype=tf.int32)\n",
    "val_samps = tf.constant(np.load('data/iris/iris_val_samps.npy'), dtype=tf.float32)\n",
    "val_labels = tf.constant(np.load('data/iris/iris_val_labels.npy'), dtype=tf.int32)\n",
    "\n",
    "# Set some vars\n",
    "C = 3\n",
    "M = train_samps.shape[1]\n",
    "mini_batch_sz = 25\n",
    "lr = 1e-1\n",
    "max_epochs = 5000\n",
    "patience = 3\n",
    "val_every = 100  # how often (in epochs) we check the val loss/acc/early stopping\n",
    "\n",
    "# Create our test net\n",
    "tf.random.set_seed(0)\n",
    "slnet = SoftmaxNet((M,), C)\n",
    "slnet.compile(lr=lr)\n",
    "\n",
    "_, val_loss_hist, val_acc_hist, e = slnet.fit(train_samps, train_labels, val_samps, val_labels,\n",
    "                                              batch_size=mini_batch_sz,\n",
    "                                              max_epochs=max_epochs,\n",
    "                                              patience=patience,\n",
    "                                              val_every=val_every)\n",
    "\n",
    "print(75*'-')\n",
    "print(f'Iris test ended after {e} epochs with final val loss/acc of {val_loss_hist[-1]:.2f}/{val_acc_hist[-1]:.2f}')\n",
    "print(75*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
